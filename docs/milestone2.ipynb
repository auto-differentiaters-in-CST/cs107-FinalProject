{"cells":[{"cell_type":"markdown","source":"# Milestone2 Document","metadata":{"id":"B2T2PWyV-EUg","cell_id":"00000-a429f4e8-c19a-4edd-a5e9-0b7f8801da27"}},{"cell_type":"markdown","source":"## Feedback\n\nIntroduction: A nice introduction! \n\nBackground -0.5: It would be hard for users to understand automatic differentiation, computational graph, and evaluation trace if you don't give the corresponding illustrations in the Background section \n\n**Revision: provided a concrete example of evaluation trace and computational graph**\n\nHow to use -0.5: didn't show how the users can get the package from online. Is AutodiffCST the name of a python file or the package? Please give different names to avoid confusion. \n\nImplementation: Using a tree as the core data structure sounds new. It would be better if you could explain it with more details.","metadata":{"cell_id":"00001-519c022e-9aad-4014-a5b9-e95373094197"}},{"cell_type":"markdown","source":"## Section 1: Introduction\nThis package autodiffCST implements automatic differentiation. It can be used to automatically differentiate functions via forward mode and reverse mode, depending on the user's choice. It also provides an option of performing second order differentiation.\n\nDifferentiation, namely, the process of finding the derivatives of functions, is very prevalent in various areas of science and engineering. It can often be used to find the extrema of functions with single or multiple variables. With the advance of technology, more complicated functions and larger dataset are developed. The difficulty of performing differentiation has greatly increased and we are more dependent on computers to take derivates. Nowadays, we have three major ways of performing differentiation: symbolic, numerical and automatic (algorithmic) differentiation. We will focus on automatic differentiation for the rest of this document.\n\n","metadata":{"id":"XZ8-Cv7u-ZVK","cell_id":"00002-2c634bfe-bbf7-4f1a-bf80-63f65689c025"}},{"cell_type":"markdown","source":"## Section 2: Background\n### 2.1 An Overview of Auto Differentiation\nAutomatic differentiation (AD) uses algorithms to efficiently and accurately evaluating derivatives of numeric functions. It has the advantage of avoiding symbolic manipulation of functions while reaching an accuracy close to machine precision. Application of automatic differentiation includes but is not limited to astronomy, dynamic systems, numerical analysis research, optimization in finance and engineering.\n\nThe idea behind AD is to break down a function into a sequence of elementary operations and functions that have easily attained derivatives, and then sequencially apply the chain rule to evaluate the derivatives of these operations to compute the derivative of the whole function.\n\nThe two main methods of performing automatic differentiation are forward mode and reverse mode. Some other AD algorithms implement a combination of forward mode and reverse mode, but this package will implement them seperately.  \n\nTo better understand automatic differentiation, it is uncessary to get familar with some key concepts that are used in the algorithms of AD. We will use the rest of this section to briefly introduce them.\n\n### 2.2 Elementary operations and functions\nThe algorithm of automatic differentiation breaks down functions into elementary arithmetic operations and elementary functions. Elementary arithmetic operations include addition, subtraction, multiplication, division and raising power (we can also consider taking roots of a number as raising it to powers less than $1$). Elementary functions include exponential, logrithmatic, and trigonometry. All of these operations and functions mentioned here have derivates that are easy to compute, so we use them as elementary steps in the evaluation trace of AD.\n\n### 2.3 The Chain Rule\nThe chain rule can be used to calculate the derivate of nested functions, such in the form of $u(v(t))$. For this function, the derivative of $u$ with respect to $t$ is $$\\dfrac{\\partial u}{\\partial t} = \\dfrac{\\partial u}{\\partial v}\\dfrac{\\partial v}{\\partial t}.$$\n\nA more general form of chain rule applies when a function $h$ has several arguments, or when its argument is a vector. Suppose we have $h = h(y(t))$ where  $y \\in R^n$ and $t \\in R^m $. Here, $h$ is the combination of $n$ functions, each of which has $m$ variables. Using the chain rule, the derivative of $h$ with respect to $t$, now called the gradient of $h$, is\n\n $$    \\nabla_{t}h = \\sum_{i=1}^{n}{\\frac{\\partial h}{\\partial y_{i}}\\nabla y_{i}\\left(t\\right)}.$$\n\nThe chain rule enables us to break down complicated and nested functions into layers and operations. Our automatic differentiation algrithm sequencially sues chain rule to compute the derivative of funtions. \n\n### 2.4 Evaluation Trace and Computational Graph\n\nThese two concepts are the core of our automatic differentiation algorithm. Since they are so important and can be created at the same time, creating them would be the first thing to do when a function is inputted into the algorithm.\n\nThe evaluation trace tracks each layer of operations while evaluate the input function and its derivative. At each step the evaluation trace holds the traces, elementary operations, numerical values, elementary derivatives and partial derivatives. \n\nThe computational graph is a graphical visualization of the evaluation trace. It holds the traces and elementary operations of the steps, connecting them via arrows pointing from input to output for each step. The computational graph helps us to better understand the structure of the function and its evaluation trace. Forward mode performs the operations from the start to the end of the graph or evaluation trace. Reverse mode performs the operations backwards, while applying the chain rule at each time determining the derivate of the trace.\n\nHere, we provide an example of a evaluation trace and a computational graph of the function $f(x,y)=exp(−(sin(x)−cos(y))^2)$, with derivatives evaluated at $f(π/2,π/3)$.\n","metadata":{"id":"wbBzU9L9-dUk","cell_id":"00003-e78121a0-9707-47b7-a444-f94d651342f4"}},{"cell_type":"markdown","source":"Evaluation trace:\n\n|Trace|Elementary Function| &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Current Value &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|Elementary Function Derivative| &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;$\\nabla_x$ &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;$\\nabla_y$ &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;|\n| :---:   | :-----------: | :-------:  | :-------------:       | :----------: | :-----------: |\n| $x_{1}$ | $x_{1}$       | $\\frac{\\pi}{2}$   | $\\dot{x}_{1}$           | $1$         | $0$          |\n| $y_{1}$ | $y_{1}$       | $\\frac{\\pi}{3}$   | $\\dot{y}_{1}$           | $0$         | $1$          |\n| $v_{1}$ | $sin(x_{1})$  | $1$    | $cos(x_{1})\\dot{x}_{1}$ | $0$         | $0$          |\n| $v_{2}$ | $cos(y_{1})$  | $0.5$  | $-sin(y_{1})\\dot{y}_{1}$| $0$         | $-0.866$         |\n| $v_{3}$ | $v_{1}-v_{2}$ | $0.5$ | $\\dot{v}_{1}-\\dot{v}_{2}$| $0$        | $0.866$          |\n| $v_{4}$ | $v_{3}^2$     | $0.25$ | $2v_{3}\\dot{v}_{3}$     | $0$         | $0.866$          |\n| $v_{5}$ | $-v_{4}$      | $-0.25$| $-\\dot{v}_{4}$           | $0$         | $-0.866$         |\n| $v_{6}$ | $exp(v_{5})$  | $0.779$| $exp(v_{5})\\dot{v}_{5}$ | $0$         | $-0.6746$        |\n| $f$     | $v_{6}$       | $0.779$| $\\dot{v}_{6}$           | $0$         | $-0.6746$        |\n\nComputational graph:","metadata":{"cell_id":"00004-a779d66d-dd31-45e2-8c60-545594c8d932"}},{"cell_type":"markdown","source":"![2.4 Graph](C_graph_example.jpg \"Computational Graph\")","metadata":{"cell_id":"00005-3ddccf18-c84d-45c0-b7dc-dc05809d431f"}},{"cell_type":"markdown","source":"## Section 3: How to Use AutodiffCST\n\n**add instructions for installation**","metadata":{"id":"2QbWtCrE75wE","cell_id":"00006-aa673c45-bac1-40e8-aaae-afa8ffe6f743"}},{"cell_type":"markdown","source":"Users should import the package or the class autodiffCST, and simply initiate the AD object by giving the function (or a collection of functions in a list form, representing a matrix of functions) they wish to differentiate. They will then be able to give a point and a direction and then access the derivative(s), along with some other supplementary features as  in the code demo provided below.\n","metadata":{"cell_id":"00007-bf8e7843-ae69-4426-aad3-fe40ae0acb71"}},{"cell_type":"code","metadata":{"id":"8cfjiepW8Hxs","cell_id":"00008-04bedcd9-61cb-4043-8dec-63193c569d73"},"source":"# import modules\nimport numpy as np\nfrom AutodiffCST import AutodiffCST as adcst\n\n# user's function, point and direction for differentiation\ninputfunction = ... # should be a list of functions, can include one or more\npoint = ...     # the point at which we want the derivatives\ndirection = ...   # the direction p in our Jp\n\n# call the class and initialize AutodiffCST object\nAD = adcst(inputfunction)\n\n\n\"\"\"\nThe following is the core.\nIt will get a list of values, which are the derivatives at the given point of the input functions, respectively.\n- AD_type specifies the mode, the default value will be True, representing the forward mode, and False will use the reverse mode.\n- tracetable gives the option of printing out a complete trace table for the computation. \n  If we say True, then we will get the table. The default setting is False as the table might be very long.\n\"\"\"\nAD.diff(direction, point, AD_type=True, tracetable=False)\n\n\n# We could also see the graph of i-th function in our input.\n# If we don't specify, then all graphs will be returned\nAD.graph()[i]\n\n\n\"\"\"\nAn option to add a new function to the input function list.\nFor example, suppose we originally have [xy,y-3].\nWe can use this function to include a third component and thus have a new function, say [xy,y-3,x-2]\n\"\"\"\nAD.add_function(new_function)\n# Then we should do AD.diff again to obtain derivatives for the updated function matrix\n\n# get the value of the function at the point\nAD.eval(point)\n\n# These are the most important features for our basic AD. Might add more later ...","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Section 4: Software Organization\nThe home directory of our software package would be structured as follows.\n\n- LICENSE\n- README.md\n- doc/\n * quickstart_tutotial.md\n * model_documentation.md\n * testing_guidelines.md\n * concepts_explanation.md\n * references.md\n- setup.py\n- src/\n * \\_\\_init\\_\\_.py\n * Tree.py\n * AutodiffCST.py\n * Forward.py\n * Reverse.py\n * Extension.py\n\n- tests/\n * test_core.py\n * test_extension.py\n\n- TravisCI.yml\n- CodeCov.yml\n\n\nSpecificly speaking, the README file would contain a general package description and the necessary information for users to navigate in the subdirectories. Besides, we would place our documentation, testing guidelines, a simple tutorial and relative references in the doc directory. Moreover, to package our model with PyPI, we need to include setup.py and a src directory, where stores the source code about our model. Furthermore, we would put a collection of test cases in tests directory. Last but not least, we would include TravisCI.yml and CodeCov.yml in our home directory for integrated test.\n\nIn this package, we plan to use the following public modules. \n\n- Modules for mathmatical calculation:\n  * Numpy: we would use it for matrix operations, and basic math functions and values, such as sin, cos, \\pi, e, etc. \n\n- Modules for testing:\n  * pydoc\n  * doctest \n  * Pytest\n\n- Other modules:\n  * sys\n  * setuptools: we would use is for publishing our model with PyPI. \n \nTo distribute our package, we would use PyPI so that users could easily install the package with *pip install*.\n\nTo better organize our software, we plan to use PyScaffold and Sphinx. The former could help us setting up the project while the latter would polish our documentation. \n ","metadata":{"id":"tTltiL5H-g-4","cell_id":"00009-2b529d0a-7e8c-477b-b625-14c875ba5cbb"}},{"cell_type":"markdown","source":"## Section 5: Implementation","metadata":{"cell_id":"00010-f1101182-f603-4100-87d6-85067323a8ea"}},{"cell_type":"markdown","source":"1. **use list as the main data structures (like numpy)**\n2. **add \"rewrite dunder methods\" for forward mode**","metadata":{"cell_id":"00011-efe90a40-d41d-4f0d-a4f0-1083cf2ad1bc"}},{"cell_type":"markdown","source":"\n\nTo implement the forward mode of automatic differentiation, we create a list for each function to represent the graph and then follow the graph to perform calculations. We plan to implement the following class: \n \n  1. Forward: calculate derivatives by forward mode.\n  1. Reverse: calculate derivatives by reverse mode.\n  1. AutodiffCST: main class, accept input functions and return derivatives at a specific point and direction. It will also return second derivatives, graphs, and trace tables, as users need. \n  1. Extension: apply our software to implement a useful extension or application.\n \nFor each class, there are possible attributes as following:\n\n  1. Forward: \n\n    - Name attributes: lists representing input functions, point (at which the derivative will be calculated), direction (vector p). \n    - Method: diff(): accept all name attributes as parameters and calculate derivatives by forward mode. \n  1. Reverse: \n\n    - Name attributes: lists representing input functions, point (at which the derivative will be calculated), direction (vector p). \n    - Method: diff(): accept all name attributes as parameters and calculate derivatives by reverse mode. \n  1. AutodiffCST: \n\n    - Name attributes: input functions\n    - Method: \n      - to_list(input function): factor a input function to a series of basic operations/functions and generate a list to represent the process.\n      - diff(direction, point, AD_type, tracetable=True/False): calculate derivatives of input functions given specific direction and point. Users could indicate calculation mode by setting AD_type, and choose whether to return tracetable by setting True/False. \n      - graph(): return graph(s) of input function(s).\n      - add_function(function): users can append a new function to the original list of input functions. \n\n  1. Extension: To be determined.\n\nIn our implementation, we would use some external dependencies such as Numpy and Math. To deal with elementary functions, we would allow users to enter functions that can be recognized by Python, factor a input function to a series of basic operations/functions (such as sin, sqrt, log, and exp) and use if-statements to check functions and return their symbolic derivatives. We allow users to enter a list of functions, which can take several variables. In addition, we take functions as input when initializing a AD object, which allow us to save the graphs in memory and calculate derivatives quickly without re-processing the functions. \n","metadata":{"id":"RPdZguY4-md3","cell_id":"00012-a8811473-0ba9-447b-9d0d-96cd0b00d6f9"}},{"cell_type":"markdown","source":"Forword Mode(more details)\n\n1. constructor\n\n``` python\ndef __init__(self, val, der = 1.0, tag, mode = \"forward\"):\n    self.val = val\n    self.der = der\n    self.tag = tag\n    self.mode = mode\n```       \n2. overload dunder methods:\n\n``` python\n__add__\n__sub__\n__pow__\n__mul__\n__mod__\n__div__\n__iadd__\n\n``` \n(see more https://www.python-course.eu/python3_magic_methods.php)\n\n\n3. Handle exceptions appropriately.\n    This is a good place to use (and practice) duck-typing. For example, rather than checking if an argument to a special method is an instance of the object, instead use a try-except block, catch an AttributeError and do the appropriate calculation.\nHint: Asking Forgiveness","metadata":{"cell_id":"00013-5b00d642-df99-4e93-8fe9-ad6504520c8e"}},{"cell_type":"markdown","source":"# Tasks","metadata":{"id":"Hhbq-6cYVQ5A","cell_id":"00014-38521e99-ac04-40d6-ba62-c2c4565e7492"}},{"cell_type":"markdown","source":"1. Main Functions\n2. Test Suite\n3. User Interface","metadata":{"cell_id":"00015-3abaf79e-ed44-42f3-9ca6-987724d81f07"}},{"cell_type":"markdown","source":"# Building Timeline","metadata":{"cell_id":"00016-ae938266-d588-44fe-8cf0-0f1e8037a12f"}},{"cell_type":"markdown","source":"- Nov.3: Finish M2A and M2B\n- Nov.7: Finish basics dunder methods for one variable\n- Nov.14: Finish Test Suite\n- Nov.20: Submit M2","metadata":{"cell_id":"00017-5fee3136-d6de-4d65-bd35-a497620fd80c"}}],"nbformat":4,"nbformat_minor":1,"metadata":{"colab":{"collapsed_sections":[],"name":"milestone1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"deepnote_notebook_id":"7a9561a0-8feb-4a89-9b6d-4b39a18ba7a1","deepnote_execution_queue":[]}}