{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-a429f4e8-c19a-4edd-a5e9-0b7f8801da27",
    "id": "B2T2PWyV-EUg"
   },
   "source": [
    "# Milestone2 Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-519c022e-9aad-4014-a5b9-e95373094197"
   },
   "source": [
    "## Feedback\n",
    "\n",
    "- Introduction: A nice introduction! \n",
    "\n",
    "- Background -0.5: It would be hard for users to understand automatic differentiation, computational graph, and evaluation trace if you don't give the corresponding illustrations in the Background section \n",
    "  \n",
    "  **Revision: provided a concrete example of evaluation trace and computational graph**\n",
    "\n",
    "\n",
    "- How to use -0.5: didn't show how the users can get the package from online. Is AutodiffCST the name of a python file or the package? Please give different names to avoid confusion. \n",
    "\n",
    "  **Revision: added instructions for installation, and change the python file name to AD.py**\n",
    "\n",
    "\n",
    "- Implementation: Using a tree as the core data structure sounds new. It would be better if you could explain it with more details.\n",
    "\n",
    "  **Revision: Changed core data structure to AD object, and updated the implementation part accordingly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-2c634bfe-bbf7-4f1a-bf80-63f65689c025",
    "id": "XZ8-Cv7u-ZVK"
   },
   "source": [
    "## Section 1: Introduction\n",
    "This package autodiffCST implements automatic differentiation. It can be used to automatically differentiate functions via forward mode and reverse mode, depending on the user's choice. It also provides an option of performing second order differentiation.\n",
    "\n",
    "Differentiation, namely, the process of finding the derivatives of functions, is very prevalent in various areas of science and engineering. It can often be used to find the extrema of functions with single or multiple variables. With the advance of technology, more complicated functions and larger dataset are developed. The difficulty of performing differentiation has greatly increased and we are more dependent on computers to take derivates. Nowadays, we have three major ways of performing differentiation: symbolic, numerical and automatic (algorithmic) differentiation. We will focus on automatic differentiation for the rest of this document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-e78121a0-9707-47b7-a444-f94d651342f4",
    "id": "wbBzU9L9-dUk"
   },
   "source": [
    "## Section 2: Background\n",
    "### 2.1 An Overview of Auto Differentiation\n",
    "Automatic differentiation (AD) uses algorithms to efficiently and accurately evaluating derivatives of numeric functions. It has the advantage of avoiding symbolic manipulation of functions while reaching an accuracy close to machine precision. Application of automatic differentiation includes but is not limited to astronomy, dynamic systems, numerical analysis research, optimization in finance and engineering.\n",
    "\n",
    "The idea behind AD is to break down a function into a sequence of elementary operations and functions that have easily attained derivatives, and then sequencially apply the chain rule to evaluate the derivatives of these operations to compute the derivative of the whole function.\n",
    "\n",
    "The two main methods of performing automatic differentiation are forward mode and reverse mode. Some other AD algorithms implement a combination of forward mode and reverse mode, but this package will implement them seperately.  \n",
    "\n",
    "To better understand automatic differentiation, it is uncessary to get familar with some key concepts that are used in the algorithms of AD. We will use the rest of this section to briefly introduce them.\n",
    "\n",
    "### 2.2 Elementary operations and functions\n",
    "The algorithm of automatic differentiation breaks down functions into elementary arithmetic operations and elementary functions. Elementary arithmetic operations include addition, subtraction, multiplication, division and raising power (we can also consider taking roots of a number as raising it to powers less than $1$). Elementary functions include exponential, logrithmatic, and trigonometry. All of these operations and functions mentioned here have derivates that are easy to compute, so we use them as elementary steps in the evaluation trace of AD.\n",
    "\n",
    "### 2.3 The Chain Rule\n",
    "The chain rule can be used to calculate the derivate of nested functions, such in the form of $u(v(t))$. For this function, the derivative of $u$ with respect to $t$ is $$\\dfrac{\\partial u}{\\partial t} = \\dfrac{\\partial u}{\\partial v}\\dfrac{\\partial v}{\\partial t}.$$\n",
    "\n",
    "A more general form of chain rule applies when a function $h$ has several arguments, or when its argument is a vector. Suppose we have $h = h(y(t))$ where  $y \\in R^n$ and $t \\in R^m $. Here, $h$ is the combination of $n$ functions, each of which has $m$ variables. Using the chain rule, the derivative of $h$ with respect to $t$, now called the gradient of $h$, is\n",
    "\n",
    " $$    \\nabla_{t}h = \\sum_{i=1}^{n}{\\frac{\\partial h}{\\partial y_{i}}\\nabla y_{i}\\left(t\\right)}.$$\n",
    "\n",
    "The chain rule enables us to break down complicated and nested functions into layers and operations. Our automatic differentiation algrithm sequencially sues chain rule to compute the derivative of funtions. \n",
    "\n",
    "### 2.4 Evaluation Trace and Computational Graph\n",
    "\n",
    "These two concepts are the core of our automatic differentiation algorithm. Since they are so important and can be created at the same time, creating them would be the first thing to do when a function is inputted into the algorithm.\n",
    "\n",
    "The evaluation trace tracks each layer of operations while evaluate the input function and its derivative. At each step the evaluation trace holds the traces, elementary operations, numerical values, elementary derivatives and partial derivatives. \n",
    "\n",
    "The computational graph is a graphical visualization of the evaluation trace. It holds the traces and elementary operations of the steps, connecting them via arrows pointing from input to output for each step. The computational graph helps us to better understand the structure of the function and its evaluation trace. Forward mode performs the operations from the start to the end of the graph or evaluation trace. Reverse mode performs the operations backwards, while applying the chain rule at each time determining the derivate of the trace.\n",
    "\n",
    "Here, we provide an example of a evaluation trace and a computational graph of the function $f(x,y)=exp(−(sin(x)−cos(y))^2)$, with derivatives evaluated at $f(π/2,π/3)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-a779d66d-dd31-45e2-8c60-545594c8d932"
   },
   "source": [
    "Evaluation trace:\n",
    "\n",
    "|Trace|Elementary Function| &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Current Value &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|Elementary Function Derivative| &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;$\\nabla_x$ &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;$\\nabla_y$ &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;|\n",
    "| :---:   | :-----------: | :-------:  | :-------------:       | :----------: | :-----------: |\n",
    "| $x_{1}$ | $x_{1}$       | $\\frac{\\pi}{2}$   | $\\dot{x}_{1}$           | $1$         | $0$          |\n",
    "| $y_{1}$ | $y_{1}$       | $\\frac{\\pi}{3}$   | $\\dot{y}_{1}$           | $0$         | $1$          |\n",
    "| $v_{1}$ | $sin(x_{1})$  | $1$    | $cos(x_{1})\\dot{x}_{1}$ | $0$         | $0$          |\n",
    "| $v_{2}$ | $cos(y_{1})$  | $0.5$  | $-sin(y_{1})\\dot{y}_{1}$| $0$         | $-0.866$         |\n",
    "| $v_{3}$ | $v_{1}-v_{2}$ | $0.5$ | $\\dot{v}_{1}-\\dot{v}_{2}$| $0$        | $0.866$          |\n",
    "| $v_{4}$ | $v_{3}^2$     | $0.25$ | $2v_{3}\\dot{v}_{3}$     | $0$         | $0.866$          |\n",
    "| $v_{5}$ | $-v_{4}$      | $-0.25$| $-\\dot{v}_{4}$           | $0$         | $-0.866$         |\n",
    "| $v_{6}$ | $exp(v_{5})$  | $0.779$| $exp(v_{5})\\dot{v}_{5}$ | $0$         | $-0.6746$        |\n",
    "| $f$     | $v_{6}$       | $0.779$| $\\dot{v}_{6}$           | $0$         | $-0.6746$        |\n",
    "\n",
    "Computational graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-3ddccf18-c84d-45c0-b7dc-dc05809d431f"
   },
   "source": [
    "![2.4 Graph](C_graph_example.jpg \"Computational Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-aa673c45-bac1-40e8-aaae-afa8ffe6f743",
    "id": "2QbWtCrE75wE"
   },
   "source": [
    "## Section 3: How to Use AutodiffCST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installation**\n",
    "\n",
    "Our package is for Python 3 only. To install AutodiffCST, you need to have pip3 installed first. If you don't, please install pip3 following these instructions https://pip.pypa.io/en/stable/installing/.\n",
    "\n",
    "Then, you could install this package by running \n",
    "```pip3 install AutodiffCST``` from the command line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-bf8e7843-ae69-4426-aad3-fe40ae0acb71"
   },
   "source": [
    "**User Guide**\n",
    "\n",
    "After installation, users could import this package by ```from AutodiffCST import AD```.\n",
    "\n",
    "Then, they could simply initiate the AD object by giving the point where they wish to differentiate. Moreover, they could also try other supplementary features as in the code demo provided below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-04bedcd9-61cb-4043-8dec-63193c569d73",
    "id": "8cfjiepW8Hxs"
   },
   "source": [
    "``` python\n",
    "# import modules\n",
    "import numpy as np\n",
    "from AutodiffCST import AD as ad\n",
    "\n",
    "# base case: initialize AD object with scalar values\n",
    "\n",
    "x = ad(5, tag = \"x\") # initialize AD object called \"x\" with the value 5\n",
    "y = ad(3, tag = \"y\") # initialize AD object called \"y\" with the value 3\n",
    "\n",
    "f = x*y + 1          # build a function with AD objects, the function will also be an AD object\n",
    "print(f)             # print 9.0\n",
    "\n",
    "dfdx = f.diff(x, trace=True) # returns the derivative with respect to x\n",
    "print(dfdx)                  # print 3.0\n",
    "\n",
    "df = f.gradient([x,y], trace=True) # returns a vector of partial derivative with respect to x and y\n",
    "print(df)                          # print [3.0,5.0]\n",
    "\n",
    "## If trace = True\n",
    "f.trace_table(func=f)             # print the trace table for f\n",
    "f.graph(func=f)                   # print the computational graph for f\n",
    "\n",
    "\n",
    "# advanced case: initialize multiple AD objects with vectors\n",
    "\n",
    "x, y, z = ad([1,2,3], tag = \"groupA\") # initialize AD objects x, y, z with values 1, 2, 3 respectively\n",
    "f1,f2,f3= x+y, x**2+z, x*y*z          # build three functions with x, y, z\n",
    "\n",
    "jacobian([f1, f2, f3], [x, y, z])     # return jacobian matrix \n",
    "\n",
    "\n",
    "# These are the most important features for our forward AD. Would add more later ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-2b529d0a-7e8c-477b-b625-14c875ba5cbb",
    "id": "tTltiL5H-g-4"
   },
   "source": [
    "## Section 4: Software Organization\n",
    "The home directory of our software package would be structured as follows.\n",
    "\n",
    "- LICENSE\n",
    "- README.md\n",
    "- doc/\n",
    " * quickstart_tutotial.md\n",
    " * model_documentation.md\n",
    " * testing_guidelines.md\n",
    " * concepts_explanation.md\n",
    " * references.md\n",
    "- setup.py\n",
    "- src/\n",
    " * \\_\\_init\\_\\_.py\n",
    " * AD.py\n",
    " * Extension.py\n",
    "\n",
    "- tests/\n",
    " * test_core.py\n",
    " * test_extension.py\n",
    "\n",
    "- TravisCI.yml\n",
    "- CodeCov.yml\n",
    "\n",
    "\n",
    "Specificly speaking, the README file would contain a general package description and the necessary information for users to navigate in the subdirectories. Besides, we would place our documentation, testing guidelines, a simple tutorial and relative references in the doc directory. Moreover, to package our model with PyPI, we need to include setup.py and a src directory, where stores the source code about our model. Furthermore, we would put a collection of test cases in tests directory. Last but not least, we would include TravisCI.yml and CodeCov.yml in our home directory for integrated test.\n",
    "\n",
    "In this package, we plan to use the following public modules. \n",
    "\n",
    "- Modules for mathmatical calculation:\n",
    "  * Numpy: we would use it for matrix operations, and basic math functions and values, such as sin, cos, \\pi, e, etc. \n",
    "\n",
    "- Modules for testing:\n",
    "  * pydoc\n",
    "  * doctest \n",
    "  * Pytest\n",
    "\n",
    "- Other modules:\n",
    "  * sys\n",
    "  * setuptools: we would use is for publishing our model with PyPI. \n",
    " \n",
    "To distribute our package, we would use PyPI so that users could easily install the package with *pip install*.\n",
    "\n",
    "To better organize our software, we plan to use PyScaffold and Sphinx. The former could help us setting up the project while the latter would polish our documentation. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-f1101182-f603-4100-87d6-85067323a8ea"
   },
   "source": [
    "## Section 5: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-a8811473-0ba9-447b-9d0d-96cd0b00d6f9",
    "id": "RPdZguY4-md3"
   },
   "source": [
    "Our main data structure is the AD object, which has the attributes of a value, a derivative and a tag. In terms of the classes, our main class is the AD object, and we would probably have several heritaged class for our extensions.\n",
    "\n",
    "In the AD class, we would have the following methods:\n",
    "\n",
    "- a constructor\n",
    "\n",
    "``` python\n",
    "def __init__(self, val, der=1, tag, mode = \"forward\"):\n",
    "    self.val = val\n",
    "    self.der = der\n",
    "    self.tag = tag\n",
    "    self.mode = mode\n",
    "```  \n",
    "- overloaded dunder methods as follows:\n",
    "\n",
    "``` python\n",
    "__add__\n",
    "__sub__\n",
    "__pow__\n",
    "__mul__\n",
    "__mod__\n",
    "__div__\n",
    "__iadd__\n",
    "``` \n",
    "&ensp; and more basic operations according to https://www.python-course.eu/python3_magic_methods.php\n",
    "\n",
    "- a diff method, which takes in a direction, and returns the derivative of the function.\n",
    "\n",
    "``` python\n",
    "def diff(self, dir = x):\n",
    "    if isinstance(dir, AD):\n",
    "        return self.der[dir]\n",
    "    else:\n",
    "        return 0\n",
    "```  \n",
    "\n",
    "- a gradient method, which takes in a vector of directions, and returns a vector of the partial derivatives at each direction.\n",
    "\n",
    "- a jacobian method, which takes in a vector of AD functions and a vector of directions, and returns the jacobian matrix.\n",
    "\n",
    "- a trace_table method, which takes in a function, and returns the trace table for that function. \n",
    "\n",
    "- A graph method, which takes in a function, and returns the computational graph of the function.\n",
    "\n",
    "\n",
    "\n",
    "In our implementation, we would use some external dependencies such as Numpy and Math. To deal with elementary functions, we would allow users to enter functions that can be recognized by Python, factor a input function to a series of basic operations/functions (such as sin, sqrt, log, and exp) and use if-statements to check functions and return their symbolic derivatives. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-38521e99-ac04-40d6-ba62-c2c4565e7492",
    "id": "Hhbq-6cYVQ5A"
   },
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-3abaf79e-ed44-42f3-9ca6-987724d81f07"
   },
   "source": [
    "1. Main Functions\n",
    "2. Test Suite\n",
    "3. User Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-ae938266-d588-44fe-8cf0-0f1e8037a12f"
   },
   "source": [
    "# Building Timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00017-5fee3136-d6de-4d65-bd35-a497620fd80c"
   },
   "source": [
    "- Nov.4: Finish M2A and M2B\n",
    "- Nov.7: Finish basics dunder methods for one variable\n",
    "- Nov.14: Finish Test Suite\n",
    "- Nov.20: Submit M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "milestone1.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "7a9561a0-8feb-4a89-9b6d-4b39a18ba7a1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
