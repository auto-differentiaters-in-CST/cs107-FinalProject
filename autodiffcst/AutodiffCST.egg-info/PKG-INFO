Metadata-Version: 2.1
Name: autodiffCST
Version: 0.0.post0.dev183+g37d678a.dirty
Summary: This package autodiffCST implements automatic differentiation. Users could perform forward mode, and use it for higher order differentiation.
Home-page: https://github.com/auto-differentiaters-in-CST/cs107-FinalProject
Author: Xiaohan Yang, Hanwen Zhang, Runting Yang, Max Li
Author-email: xiaohan_yang@g.harvard.edu, hzhang1@g.harvard.edu, runting_yang@hsph.harvard.edu, manli@fas.harvard.edu
License: mit
Project-URL: Documentation, https://github.com/auto-differentiaters-in-CST/cs107-FinalProject/tree/master/docs
Description: # cs107-FinalProject
        ## This is the repository for CS107 group project for fall 2020 at HU.
        ### Group 5
        Contributors: Xiaohan Yang, Max Li, Runting Yang,Hanwen Zhang
        
        travis badge:
        [![Build Status](https://travis-ci.com/auto-differentiaters-in-CST/cs107-FinalProject.svg?token=AjVcVSqkqdiJgwaimWYR&branch=master)](https://travis-ci.com/auto-differentiaters-in-CST/cs107-FinalProject)
        
        codecov badge:
        [![codecov](https://codecov.io/gh/auto-differentiaters-in-CST/cs107-FinalProject/branch/master/graph/badge.svg?token=US1Y8Z9OE0)](https://codecov.io/gh/auto-differentiaters-in-CST/cs107-FinalProject)
        
        
        ## Introduction
        This package autodiffCST implements automatic differentiation. It would be used to automatically differentiate functions via forward mode and reverse mode, depending on the user's choice. It would also provides an option of performing second order differentiation. 
        
        Differentiation, namely, the process of finding the derivatives of functions, is very prevalent in various areas of science and engineering. It can often be used to find the extrema of functions with single or multiple variables. With the advance of technology, more complicated functions and larger dataset are developed. The difficulty of performing differentiation has greatly increased and we are more dependent on computers to take derivates. Nowadays, we have three major ways of performing differentiation: symbolic, numerical and automatic (algorithmic) differentiation. We will focus on automatic differentiation for the rest of this document.
        
        **Note: The current version could only implement the first order differentiation for one function with multiple variables.**
        
        ## Installation
        
        Our package is for Python 3 only. The current version is not released on PyPI yet, so please clone this repository to use our package. 
        
        * Clone the package repository: ```git clone https://github.com/auto-differentiaters-in-CST/cs107-FinalProject.git```
        * Change into the new directory: ```cd cs107-FinalProject``` .
        * Install the dependencies using ```pip install -r requirements.txt```.
        
        ## User Guide
        
        After installation, users could import this package by ```from autodiffcst import AD```.
        
        Then, they could simply initiate the AD object by giving the point where they wish to differentiate. Moreover, they could also try other supplementary features as in the code demo provided below.
        
        ``` python
        # import modules
        from autodiffcst import AD as ad
        from autodiffcst import admath as admath
        
        # base case: initialize AD object with scalar values
        
        x = ad.AD(5, tag = "x") # initialize AD object called "x" with the value 5
        y = ad.AD(3, tag = "y") # initialize AD object called "y" with the value 3
        
        f1 = x*y              # build a function with AD objects, the function will also be an AD object
        print(f1)             # print AD(value: {15}, derivatives: {'x': 3, 'y': 5})
        
        dfdx = f1.diff("x") # returns the derivative with respect to x
        print(dfdx)                  # print 3
         
        jacobian = ad.jacobian(f1) # returns a gradient vector of f
        print(jacobian)  # print [5,3]
        
        f2 =  x + admath.sin(y)   # build a function with AD objects
        print(f2)             # print AD(value: 5.141120008059867, derivatives: {'x': 1, 'y': -0.9899924966004454})
        
        dfdy = f2.diff("y") # returns the derivative with respect to x
        print(dfdy)                  # print -0.9899924966004454
         
        jacobian2 = ad.jacobian(f2) # returns a gradient vector of f
        print(jacobian2)  # print [1, -0.9899924966004454]
        ```
Platform: any
Classifier: Development Status :: 4 - Beta
Classifier: Programming Language :: Python
Description-Content-Type: text/x-rst; charset=UTF-8
Provides-Extra: testing
