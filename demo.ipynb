{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-a429f4e8-c19a-4edd-a5e9-0b7f8801da27",
    "id": "B2T2PWyV-EUg"
   },
   "source": [
    "# Project Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-2c634bfe-bbf7-4f1a-bf80-63f65689c025",
    "id": "XZ8-Cv7u-ZVK"
   },
   "source": [
    "## Introduction and Background\n",
    "0. Package name: autodiffCST \n",
    "1. Differentiation and Automatic Differentiation\n",
    "2. Evaluation Trace and Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-3ddccf18-c84d-45c0-b7dc-dc05809d431f"
   },
   "source": [
    "Here, we provide an example of a evaluation trace and a computational graph of the function $$f(x,y)=\\exp (-(\\sin (x)-\\cos (y))^2),$$\n",
    "with derivatives evaluated at point $(\\pi /2,\\pi /3)$.\n",
    "\n",
    "Computational graph:\n",
    "![2.4 Graph](./docs/C_graph_example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-a779d66d-dd31-45e2-8c60-545594c8d932"
   },
   "source": [
    "Evaluation trace:\n",
    "\n",
    "|Trace|Elementary Function| &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Current Value &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|Elementary Function Derivative| &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;$\\nabla_x$ &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;$\\nabla_y$ &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;|\n",
    "| :---:   | :-----------: | :-------:  | :-------------:       | :----------: | :-----------: |\n",
    "| $x_{1}$ | $x_{1}$       | $\\frac{\\pi}{2}$   | $\\dot{x}_{1}$           | $1$         | $0$          |\n",
    "| $y_{1}$ | $y_{1}$       | $\\frac{\\pi}{3}$   | $\\dot{y}_{1}$           | $0$         | $1$          |\n",
    "| $v_{1}$ | $sin(x_{1})$  | $1$    | $cos(x_{1})\\dot{x}_{1}$ | $0$         | $0$          |\n",
    "| $v_{2}$ | $cos(y_{1})$  | $0.5$  | $-sin(y_{1})\\dot{y}_{1}$| $0$         | $-0.866$         |\n",
    "| $v_{3}$ | $v_{1}-v_{2}$ | $0.5$ | $\\dot{v}_{1}-\\dot{v}_{2}$| $0$        | $0.866$          |\n",
    "| $v_{4}$ | $v_{3}^2$     | $0.25$ | $2v_{3}\\dot{v}_{3}$     | $0$         | $0.866$          |\n",
    "| $v_{5}$ | $-v_{4}$      | $-0.25$| $-\\dot{v}_{4}$           | $0$         | $-0.866$         |\n",
    "| $v_{6}$ | $exp(v_{5})$  | $0.779$| $exp(v_{5})\\dot{v}_{5}$ | $0$         | $-0.6746$        |\n",
    "| $f$     | $v_{6}$       | $0.779$| $\\dot{v}_{6}$           | $0$         | $-0.6746$        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension: Higher Order Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-f1101182-f603-4100-87d6-85067323a8ea"
   },
   "source": [
    "## Software Organization \n",
    "Structure of the home directory:\n",
    "\n",
    "- LICENSE.txt\n",
    "- README.md\n",
    "- requirements.txt\n",
    "- docs/\n",
    "    * README.md\n",
    "    * milestone1.ipynb\n",
    "    * milestone2.ipynb\n",
    "    * milestone2_progress.ipynb\n",
    "    * documentation.ipynb\n",
    "    * documentation.md\n",
    "    * api\n",
    "    * using_VAD_for_Newtons_method.ipynb\n",
    " \n",
    "- setup.py\n",
    "- demo.ipynb\n",
    "- src/\n",
    "    - autodiffcst/\n",
    "        * \\_\\_init\\_\\_.py\n",
    "        * AD.py\n",
    "        * AD_vec.py\n",
    "        * admath.py\n",
    "\n",
    "- tests/\n",
    "    * AD_test.py\n",
    "    * test_admath.py\n",
    "\n",
    "- TravisCI.yml\n",
    "- CodeCov.yml\n",
    "\n",
    "We use PyPI to distribute our package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "0. VAD and AD\n",
    "1. Dunder Methods and Elementary Functions\n",
    "2. diff, Jacobian and Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-aa673c45-bac1-40e8-aaae-afa8ffe6f743",
    "id": "2QbWtCrE75wE"
   },
   "source": [
    "## How to Use autodiffCST\n",
    "\n",
    "0. Install pip and required packages\n",
    "1. ```pip3 install autodiffCST```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple case: a list of a single scalar variable. First-order, second-order, and higher-order derivatives can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from src.autodiffcst.AD_vec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[u] = VAD([5])           # initialize VAD objects u with a single point at 5\n",
    "\n",
    "f = u*2-3                    # build a function with VAD object\n",
    "\n",
    "print(f)                     # AD(value: [7], derivatives: [2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdu = f.diff(0)             # get derivative in the direction of u\n",
    "print(dfdu)                  # 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced cases: initialize VAD objects with vectors (multiple input values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = VAD([1,2,3])     # initialize VAD objects x, y, z with values 1, 2, 3 respectively\n",
    "                                   # with multiple variable, you can skip brackets\n",
    "\n",
    "f1,f2,f3= x+y, x**2+z, x*y*z   # build three functions with x, y, z\n",
    "print(f3)                      # AD(value: [6], tag: [0 1 2], derivatives: [6. 3. 2.])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian([f1, f2, f3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tricky case: using **VAD** to create a vector variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = VAD([1,2,3])        # initialize VAD objects: a vector v of value [1,2,3]\n",
    "    \n",
    "f = sin(v)              # build VAD: a single function applied to the vector v\n",
    "print(f)                    # print f's value and derivative. Here the second derivative will appear as a 3x3 matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.diff(0,order=1)          # get the first derivative with respect to v[0] (or x0), the first variable         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-38521e99-ac04-40d6-ba62-c2c4565e7492",
    "id": "Hhbq-6cYVQ5A"
   },
   "source": [
    "## Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher-Order Derivatives and the Hessian Matrix\n",
    "\n",
    "0. The *Faa di Bruno Formula* -- a generalization of the chain rule\n",
    "\n",
    "$$\\frac{d^n}{dx^n}f(g(x))=\\sum^n_{k=1}f^{(k)}(g(x))B_{n,k}\\left(g^{(1)}(x),g^{(2)}(x),...,g^{(n-k+1)}(x)\\right),$$\n",
    "\n",
    "where the $B_{n,k}$ is the Bell polynomials\n",
    "\n",
    "1. Generalization of the product rule, the *Leibniz Rule*,\n",
    "$$\\frac{d^n}{dx^n}\\left(f(x)g(x)\\right)=\\sum^n_{k=1}\\begin{pmatrix} n\\\\k \\end{pmatrix} f^{(k)}(x)g^{(n-k)}(x).$$\n",
    "\n",
    "2. The Hessian matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jacobian for a scalar-valued function of multiple scalar variables, say $f(x_1,x_2,\\ldots,x_n)$:\n",
    "\\begin{align*}\n",
    "{J}(f)= \\begin{bmatrix}\n",
    "    \\frac{\\partial f}{\\partial x_1}, &\n",
    "     \\frac{\\partial f}{\\partial x_2},&\n",
    "    \\cdots,&\n",
    "    \\frac{\\partial f}{\\partial x_n}\\\\\n",
    "  \\end{bmatrix}. \n",
    "\\end{align*}\n",
    "And then we observe that Hessian matrix looks like \n",
    "\\begin{align*}\n",
    "{H}(f)= \\begin{bmatrix}\n",
    "    \\frac{\\partial^2f}{\\partial x^2_1} &\\frac{\\partial^2f}{\\partial x_1\\partial x_2} &\\cdots &\\frac{\\partial^2f}{\\partial x_1\\partial x_n} \\\\\n",
    "      \\frac{\\partial^2f}{\\partial x_2\\partial x_1} &\\frac{\\partial^2f}{\\partial x^2_2} &\\cdots &\\frac{\\partial^2f}{\\partial x_2\\partial x_n} \\\\\n",
    "    \\vdots  &\\vdots &\\ddots &\\vdots \\\\\n",
    "    \\frac{\\partial^2f}{\\partial x_n\\partial x_1} &\\frac{\\partial^2f}{\\partial x_n\\partial x_2} &\\cdots &\\frac{\\partial^2f}{\\partial x^2_n} \\\\\n",
    "  \\end{bmatrix}. \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more complicated case, when $f = \\begin{bmatrix}f_1(x_1,x_2,\\ldots,x_n),&f_2(x_1,x_2,\\ldots,x_n),&\\cdots,&f_m(x_1,x_2,\\ldots,x_n)\\end{bmatrix}$, we will now have the Jacobian\n",
    "\\begin{align*}\n",
    "{J}(f)= \\begin{bmatrix}\n",
    "    \\frac{\\partial f_1}{\\partial x_1} &\n",
    "     \\frac{\\partial f_1}{\\partial x_2}&\n",
    "    \\cdots &\n",
    "    \\frac{\\partial f_1}{\\partial x_n}\\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} &\n",
    "     \\frac{\\partial f_2}{\\partial x_2}&\n",
    "    \\cdots&\n",
    "    \\frac{\\partial f_2}{\\partial x_n}\\\\\n",
    "    \\vdots  &\\vdots &\\ddots &\\vdots \\\\\n",
    "    \\frac{\\partial f_m}{\\partial x_1} &\n",
    "     \\frac{\\partial f_m}{\\partial x_2}&\n",
    "    \\cdots &\n",
    "    \\frac{\\partial f_m}{\\partial x_n}\\\\\n",
    "  \\end{bmatrix}, \n",
    "\\end{align*}\n",
    "and the Hessian will be a tensor in $3D$ according to the rules (This case is not covered)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. scalar VAD: higher order derivatives $\\rightarrow$ `higher`\n",
    "\n",
    "    Use `higher or higherdiff()`\n",
    "\n",
    "\n",
    "4. vector VAD: second order derivatives $\\rightarrow$ `der2`\n",
    "\n",
    "    Use `hessian()` to access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use the Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple case: a list of a single scalar variable. First-order, second-order, and higher-order derivatives can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[u] = VAD([5])           # initialize VAD objects u with a single point at 5\n",
    "\n",
    "f = u*2-3   \n",
    "print(f)                     # AD(value: [7], derivatives: [2.])\n",
    "\n",
    "dfdu2 = f.diff([0,0],order=2) # get second derivative df^2/dudu\n",
    "print(dfdu2)                  # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x] = VAD([2],order=10)  # initialize as before, but specify that you want to get to order up tp 10\n",
    "\n",
    "g = 2*exp(x)\n",
    "g.higherdiff(10)             # 14.7781121978613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = x**3                  # let's try another case for higher-order derivatives\n",
    "f.higher                  # array([12., 12.,  6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced cases: initialize VAD objects with vectors (multiple input values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = VAD([1,2,3])     # initialize VAD objects x, y, z with values 1, 2, 3 respectively\n",
    "                                   # with multiple variable, you can skip brackets\n",
    "\n",
    "f1,f2,f3= x+y, x**2+z, x*y*z   # build three functions with x, y, z\n",
    "print(f3)  \n",
    "\n",
    "hessian(f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tricky case: using **VAD** to create a vector variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = VAD([1,2,3])        # initialize VAD objects: a vector v of value [1,2,3]\n",
    "    \n",
    "f = sin(v)              # build VAD: a single function applied to the vector v\n",
    "print(f)                    # print f's value and derivative. Here the second derivative will appear as a 3x3 matrix \n",
    "\n",
    "f.der2                      # only in this case, you will be able to get the tensor hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Demo:\n",
    "Using autodiffcst in real case: finding minimum of Rosenbrock function with Newton's method:\n",
    "$$ f(x,y)=100(y-x^2)^2+(1-x)^2,$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# define Rosenbrock\n",
    "def Rsbrk_func(x,y):\n",
    "    return 100*(y-x**2)**2+(1-x)**2\n",
    "\n",
    "# We start at point (2,1)\n",
    "x = 2; y = 1\n",
    "tol = 10**(-8)\n",
    "stepsize = 1\n",
    "# count number of iterations\n",
    "k = 0\n",
    "# this is our intermediate point duing iterations\n",
    "x_i = x; y_i = y\n",
    "# store the path\n",
    "list_x_i = []\n",
    "list_y_i = []\n",
    "list_f_i = []\n",
    "while stepsize > tol:\n",
    "    k += 1\n",
    "    # using VAD to create variables at point (x_i,y_i)\n",
    "    [a,b] = VAD([x_i,y_i])\n",
    "    # construct the function\n",
    "    Rsbrk = 100*(b-a**2)**2+(1-a)**2\n",
    "    list_x_i.append(x_i)\n",
    "    list_y_i.append(y_i)\n",
    "    list_f_i.append(Rsbrk.val[0])\n",
    "    # Take a Newton step by solving the linear system \n",
    "    # constructed using the Hessian and gradient\n",
    "    step = np.linalg.solve(hessian(Rsbrk),-jacobian(Rsbrk))\n",
    "    x_i += step[0]\n",
    "    y_i += step[1]\n",
    "    stepsize = np.linalg.norm(step)\n",
    "print('--------')\n",
    "print(\"starting point: ({0},{1})\".format(x,y))\n",
    "print(\"iterations:\",k)\n",
    "print(\"(x,y):\",path[-1])\n",
    "print(\"f:\",list_f_i[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ = np.array(np.copy(path))\n",
    "start = np.array([x,y])\n",
    "end = np.array([list_x_i[-1],list_y_i[-1]])\n",
    "optimum = np.array([1,1])\n",
    "xmin, xmax, xstep = -2.2, 2.2, .05\n",
    "ymin, ymax, ystep = -0.5, 4, .05\n",
    "X,Y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))\n",
    "Z = Rsbrk_func(X,Y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "path = path_.T\n",
    "cp = ax.contour(X,Y,Z, levels=np.logspace(0, 5, 15), norm=LogNorm(),cmap=plt.cm.jet)\n",
    "ax.quiver(path[0,:-1], path[1,:-1], path[0,1:]-path[0,:-1], path[1,1:]-path[1,:-1], scale_units='xy', angles='xy', scale=1, width=.005,color='k')\n",
    "ax.plot(list_x_i, list_y_i,'.', color='k')\n",
    "ax.plot(*start, '.', color='m',markersize=18,label='start pt')\n",
    "ax.plot(*end, '.',color='m', markersize=18,label='end pt')\n",
    "ax.plot(*optimum, 'r*', markersize=18,alpha=0.5,label='optimum')\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$y$')\n",
    "ax.set_title('optimization path with starting point at ({0},{1})'.format(x,y),fontsize=14)\n",
    "\n",
    "ax.set_xlim((xmin, xmax))\n",
    "ax.set_ylim((ymin, ymax))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future\n",
    "\n",
    "1. Higher (>2) order derivatives for vector inputs\n",
    "\n",
    "\n",
    "2. $f = \\begin{bmatrix}f_1(x_1,x_2,\\ldots,x_n),&f_2(x_1,x_2,\\ldots,x_n),&\\cdots,&f_m(x_1,x_2,\\ldots,x_n)\\end{bmatrix}$, grid Jacobian and the Hessian will be a tensor in $3D$\n",
    "\n",
    "\n",
    "3. Improve computation efficiency: is Faa di Bruno Formula the best?\n",
    "    \n",
    "    \n",
    "4. Further applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-ae938266-d588-44fe-8cf0-0f1e8037a12f"
   },
   "source": [
    "## Broader Impact and Software Inclusivity Statement"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "milestone1.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "7a9561a0-8feb-4a89-9b6d-4b39a18ba7a1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
