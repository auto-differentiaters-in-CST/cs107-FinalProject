{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-a429f4e8-c19a-4edd-a5e9-0b7f8801da27",
    "id": "B2T2PWyV-EUg"
   },
   "source": [
    "# Project Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-2c634bfe-bbf7-4f1a-bf80-63f65689c025",
    "id": "XZ8-Cv7u-ZVK"
   },
   "source": [
    "## Introduction and Background\n",
    "0. Package name: autodiffCST \n",
    "1. Differentiation and Automatic Differentiation\n",
    "2. Evaluation Trace and Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-3ddccf18-c84d-45c0-b7dc-dc05809d431f"
   },
   "source": [
    "Here, we provide an example of a evaluation trace and a computational graph of the function $$f(x,y)=\\exp (-(\\sin (x)-\\cos (y))^2),$$\n",
    "with derivatives evaluated at point $(\\pi /2,\\pi /3)$.\n",
    "\n",
    "Computational graph:\n",
    "![2.4 Graph](./docs/C_graph_example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-a779d66d-dd31-45e2-8c60-545594c8d932"
   },
   "source": [
    "Evaluation trace:\n",
    "\n",
    "|Trace|Elementary Function| &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Current Value &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|Elementary Function Derivative| &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;$\\nabla_x$ &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;$\\nabla_y$ &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;|\n",
    "| :---:   | :-----------: | :-------:  | :-------------:       | :----------: | :-----------: |\n",
    "| $x_{1}$ | $x_{1}$       | $\\frac{\\pi}{2}$   | $\\dot{x}_{1}$           | $1$         | $0$          |\n",
    "| $y_{1}$ | $y_{1}$       | $\\frac{\\pi}{3}$   | $\\dot{y}_{1}$           | $0$         | $1$          |\n",
    "| $v_{1}$ | $sin(x_{1})$  | $1$    | $cos(x_{1})\\dot{x}_{1}$ | $0$         | $0$          |\n",
    "| $v_{2}$ | $cos(y_{1})$  | $0.5$  | $-sin(y_{1})\\dot{y}_{1}$| $0$         | $-0.866$         |\n",
    "| $v_{3}$ | $v_{1}-v_{2}$ | $0.5$ | $\\dot{v}_{1}-\\dot{v}_{2}$| $0$        | $0.866$          |\n",
    "| $v_{4}$ | $v_{3}^2$     | $0.25$ | $2v_{3}\\dot{v}_{3}$     | $0$         | $0.866$          |\n",
    "| $v_{5}$ | $-v_{4}$      | $-0.25$| $-\\dot{v}_{4}$           | $0$         | $-0.866$         |\n",
    "| $v_{6}$ | $exp(v_{5})$  | $0.779$| $exp(v_{5})\\dot{v}_{5}$ | $0$         | $-0.6746$        |\n",
    "| $f$     | $v_{6}$       | $0.779$| $\\dot{v}_{6}$           | $0$         | $-0.6746$        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension: Higher Order Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-f1101182-f603-4100-87d6-85067323a8ea"
   },
   "source": [
    "## Software Organization \n",
    "Structure of the home directory:\n",
    "\n",
    "- LICENSE.txt\n",
    "- README.md\n",
    "- requirements.txt\n",
    "- docs/\n",
    "    * README.md\n",
    "    * milestone1.ipynb\n",
    "    * milestone2.ipynb\n",
    "    * milestone2_progress.ipynb\n",
    "    * documentation.ipynb\n",
    "    * documentation.md\n",
    "    * api\n",
    "    * using_VAD_for_Newtons_method.ipynb\n",
    " \n",
    "- setup.py\n",
    "- demo.ipynb\n",
    "- src/\n",
    "    - autodiffcst/\n",
    "        * \\_\\_init\\_\\_.py\n",
    "        * AD.py\n",
    "        * AD_vec.py\n",
    "        * admath.py\n",
    "\n",
    "- tests/\n",
    "    * AD_test.py\n",
    "    * test_admath.py\n",
    "\n",
    "- TravisCI.yml\n",
    "- CodeCov.yml\n",
    "\n",
    "We use PyPI to distribute our package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "0. VAD and AD\n",
    "1. Dunder Methods and Elementary Functions\n",
    "2. diff, Jacobian and Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-aa673c45-bac1-40e8-aaae-afa8ffe6f743",
    "id": "2QbWtCrE75wE"
   },
   "source": [
    "## How to Use autodiffCST\n",
    "\n",
    "0. Install pip and required packages\n",
    "1. ```pip3 install autodiffCST```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple case: a list of a single scalar variable. First-order, second-order, and higher-order derivatives can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from src.autodiffcst.AD_vec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[u] = VAD([5])           # initialize VAD objects u with a single point at 5\n",
    "\n",
    "f = u*2-3                    # build a function with VAD object\n",
    "\n",
    "print(f)                     # AD(value: [7], derivatives: [2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdu = f.diff(0)             # get derivative in the direction of u\n",
    "print(dfdu)                  # 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced cases: initialize VAD objects with vectors (multiple input values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = VAD([1,2,3])     # initialize VAD objects x, y, z with values 1, 2, 3 respectively\n",
    "                                   # with multiple variable, you can skip brackets\n",
    "\n",
    "f1,f2,f3= x+y, x**2+z, x*y*z   # build three functions with x, y, z\n",
    "print(f3)                      # AD(value: [6], tag: [0 1 2], derivatives: [6. 3. 2.])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian([f1, f2, f3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tricky case: using **VAD** to create a vector variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = cst.VAD([1,2,3])        # initialize VAD objects: a vector v of value [1,2,3]\n",
    "    \n",
    "f = cst.sin(v)              # build VAD: a single function applied to the vector v\n",
    "print(f)                    # print f's value and derivative. Here the second derivative will appear as a 3x3 matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.diff(0,order=1)          # get the first derivative with respect to v[0] (or x0), the first variable         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple case: a list of a single scalar variable. First-order, second-order, and higher-order derivatives can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[u] = VAD([5])           # initialize VAD objects u with a single point at 5\n",
    "\n",
    "f = u*2-3   \n",
    "print(f)                     # AD(value: [7], derivatives: [2.])\n",
    "\n",
    "dfdu2 = f.diff([0,0],order=2) # get second derivative df^2/dudu\n",
    "print(dfdu2)                  # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x] = VAD([2],order=10)  # initialize as before, but specify that you want to get to order up tp 10\n",
    "\n",
    "g = 2*exp(x)\n",
    "g.higherdiff(10)             # 14.7781121978613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = x**3                  # let's try another case for higher-order derivatives\n",
    "f.higher                  # array([12., 12.,  6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced cases: initialize VAD objects with vectors (multiple input values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = VAD([1,2,3])     # initialize VAD objects x, y, z with values 1, 2, 3 respectively\n",
    "                                   # with multiple variable, you can skip brackets\n",
    "\n",
    "f1,f2,f3= x+y, x**2+z, x*y*z   # build three functions with x, y, z\n",
    "print(f3)  \n",
    "\n",
    "hessian(f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tricky case: using **VAD** to create a vector variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = cst.VAD([1,2,3])        # initialize VAD objects: a vector v of value [1,2,3]\n",
    "    \n",
    "f = cst.sin(v)              # build VAD: a single function applied to the vector v\n",
    "print(f)                    # print f's value and derivative. Here the second derivative will appear as a 3x3 matrix \n",
    "\n",
    "f.der2                      # only in this case, you will be able to get the tensor hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.4 \n",
    "Using autodiffcst in real case: finding minimum of Rosenbrock function with Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rosenbrock function (https://en.wikipedia.org/wiki/Rosenbrock_function):\n",
    "\n",
    "$$ f(x,y)=100(y-x^2)^2+(1-x)^2,$$\n",
    "\n",
    "is a common test function used for optimization algorithms. Here we use it to demonstrate a nice application of our package.\n",
    "\n",
    "Please refer to docs/using_VAD_for_Newtons_method.ipynb for the complete notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "import autodiffcst as cst\n",
    "\n",
    "# We start at point (2,1)\n",
    "x = 2; y = 1\n",
    "tol = 10**(-8)\n",
    "stepsize = 1\n",
    "# count number of iterations\n",
    "k = 0\n",
    "# this is our intermediate point duing iterations\n",
    "x_i = x; y_i = y\n",
    "# store the path\n",
    "list_x_i = []\n",
    "list_y_i = []\n",
    "list_f_i = []\n",
    "while stepsize > tol:\n",
    "    k += 1\n",
    "    # using VAD to create variables at point (x_i,y_i)\n",
    "    [a,b] = cst.VAD([x_i,y_i])\n",
    "    # construct the function\n",
    "    Rsbrk = 100*(b-a**2)**2+(1-a)**2\n",
    "    list_x_i.append(x_i)\n",
    "    list_y_i.append(y_i)\n",
    "    list_f_i.append(Rsbrk.val[0])\n",
    "    # Take a Newton step by solving the linear system \n",
    "    # constructed using the Hessian and gradient\n",
    "    step = np.linalg.solve(cst.hessian(Rsbrk),-cst.jacobian(Rsbrk))\n",
    "    x_i += step[0]\n",
    "    y_i += step[1]\n",
    "    stepsize = np.linalg.norm(step)\n",
    "print('--------')\n",
    "print(\"starting point: ({0},{1})\".format(x,y))\n",
    "print(\"iterations:\",k)\n",
    "print(\"(x,y):\",path[-1])\n",
    "print(\"f:\",list_f_i[-1])\n",
    "--------\n",
    "starting point: (2,1)\n",
    "iterations: 6\n",
    "(x,y): [1. 1.]\n",
    "f: 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-38521e99-ac04-40d6-ba62-c2c4565e7492",
    "id": "Hhbq-6cYVQ5A"
   },
   "source": [
    "# Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Second and Higher-Order Derivatives and the Hessian Matrix\n",
    "\n",
    "The second-order derivative of a function $f$ is the derivative of the derivative of $f$, and often referred to as the second derivative. Roughly speaking, the second derivative measures how the rate of change of a changing quantity. For example, the second derivative of the position of an object with respect to time is the instantaneous acceleration of the object, or the rate at which the velocity of the object is changing with respect to time. On the graph of a function, the second derivative corresponds to the curvature or concavity of the graph. The graph of a function with a positive second derivative is upwardly concave, while the graph of a function with a negative second derivative curves in the opposite way. Along this way, third, fourth, and higher-order derivatives can also be calculated and interpreted as the rate of change of the previous order. They can be calculated using the *Faa di Bruno Formula* -- a generalization of the chain rule as follows,\n",
    "\n",
    "$$\\frac{d^n}{dx^n}f(g(x))=\\sum^n_{k=1}f^{(k)}(g(x))B_{n,k}\\left(g^{(1)}(x),g^{(2)}(x),...,g^{(n-k+1)}(x)\\right),$$\n",
    "\n",
    "where the $B_{n,k}$ is the Bell polynomials\n",
    "\n",
    "$$B_{n,k}(x_1,x_2,\\ldots,x_{n-k+1})=\\sum \\frac{n!}{j_1!j_2!\\cdots j_{n-k+1}!}\\left(\\frac{x_1}{1!}\\right)^{j_1}\\left(\\frac{x_2}{2!}\\right)^{j_2}\\cdots \\left(\\frac{x_{n-k+1}}{(n-k+1)!}\\right)^{j_{n-k+1}},$$\n",
    "with the sum taken over all sequences $j_1,j_2,\\ldots,j_{n-k+1}$ of non-negative integers such that these two conditions are satisfied:\n",
    "\n",
    "- $j_1+j_2+j_3+\\cdots+j_{n-k+1}=k$,\n",
    "- $j_1+2j_2+3j_3+\\cdots+(n-k+1)j_{n-k+1}=n.$\n",
    "\n",
    "Similarly, we have a generalization of the product rule, the *Leibniz Rule*,\n",
    "$$\\frac{d^n}{dx^n}\\left(f(x)g(x)\\right)=\\sum^n_{k=1}\\begin{pmatrix} n\\\\k \\end{pmatrix} f^{(k)}(x)g^{(n-k)}(x).$$\n",
    "\n",
    "The Hessian matrix or Hessian is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field. It describes the local curvature of a function of many variables. We will use the Hessian matrix to report the second-order derivatives of the functions.\n",
    "\n",
    "Recall that for a scalar-valued function of multiple scalar variables, say $f(x_1,x_2,\\ldots,x_n)$, the Jacobian matrix summarize the first-order derivatives with respect to each variable as follows:\n",
    "\\begin{align*}\n",
    "{J}(f)= \\begin{bmatrix}\n",
    "    \\frac{\\partial f}{\\partial x_1}, &\n",
    "     \\frac{\\partial f}{\\partial x_2},&\n",
    "    \\cdots,&\n",
    "    \\frac{\\partial f}{\\partial x_n}\\\\\n",
    "  \\end{bmatrix}. \n",
    "\\end{align*}\n",
    "And then we observe that Hessian matrix looks like \n",
    "\\begin{align*}\n",
    "{H}(f)= \\begin{bmatrix}\n",
    "    \\frac{\\partial^2f}{\\partial x^2_1} &\\frac{\\partial^2f}{\\partial x_1\\partial x_2} &\\cdots &\\frac{\\partial^2f}{\\partial x_1\\partial x_n} \\\\\n",
    "      \\frac{\\partial^2f}{\\partial x_2\\partial x_1} &\\frac{\\partial^2f}{\\partial x^2_2} &\\cdots &\\frac{\\partial^2f}{\\partial x_2\\partial x_n} \\\\\n",
    "    \\vdots  &\\vdots &\\ddots &\\vdots \\\\\n",
    "    \\frac{\\partial^2f}{\\partial x_n\\partial x_1} &\\frac{\\partial^2f}{\\partial x_n\\partial x_2} &\\cdots &\\frac{\\partial^2f}{\\partial x^2_n} \\\\\n",
    "  \\end{bmatrix}. \n",
    "\\end{align*}\n",
    "\n",
    "With more complicated case, when $f = \\begin{bmatrix}f_1(x_1,x_2,\\ldots,x_n),&f_2(x_1,x_2,\\ldots,x_n),&\\cdots,&f_m(x_1,x_2,\\ldots,x_n)\\end{bmatrix}$, we will now have the Jacobian\n",
    "\\begin{align*}\n",
    "{J}(f)= \\begin{bmatrix}\n",
    "    \\frac{\\partial f_1}{\\partial x_1} &\n",
    "     \\frac{\\partial f_1}{\\partial x_2}&\n",
    "    \\cdots &\n",
    "    \\frac{\\partial f_1}{\\partial x_n}\\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} &\n",
    "     \\frac{\\partial f_2}{\\partial x_2}&\n",
    "    \\cdots&\n",
    "    \\frac{\\partial f_2}{\\partial x_n}\\\\\n",
    "    \\vdots  &\\vdots &\\ddots &\\vdots \\\\\n",
    "    \\frac{\\partial f_m}{\\partial x_1} &\n",
    "     \\frac{\\partial f_m}{\\partial x_2}&\n",
    "    \\cdots &\n",
    "    \\frac{\\partial f_m}{\\partial x_n}\\\\\n",
    "  \\end{bmatrix}, \n",
    "\\end{align*}\n",
    "and the Hessian will be a tensor in $3D$ according to the rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-3abaf79e-ed44-42f3-9ca6-987724d81f07"
   },
   "source": [
    "Here is the future features section from *Milestone 2*:\n",
    "\n",
    "1. Differentiate a list of functions. Our package can deal with one function with multiple varaibles. In the future we plan to take a list of functions as input and output its Jacobian accordingly. Using Numpy array as the data structure to keep the Jacobian would be ideal, so we will need to change the implementation of our current jacobian method. \n",
    "\n",
    "2. Backward Mode. Right now our mode for doing automatic differetiation is defaulted to forward mode, because we have not implemented backward mode yet. We would need new functions that use the AD object class to implement backward mode. To keep track of the traces, we need to create a trace table, possibly using Numpy array, in the function that runs backward mode. \n",
    "\n",
    "3. Newton's method. We would like to use our AD package to solve meaningful problems. One way to achieve this is to use it in an implementation of Newton's method. This will be a script that imports our AD package to calculate the derivatives in Newton's method.\n",
    "\n",
    "After some consideration, we decided that we would like to do ***higher order derivatives*** instead of Backward Mode and Newton's method as proposed. This pivot shifting is approved, so we updated the future features section to reflect this change and M2 feedback (the update was made before the whole module was finished so some function names and implementations might not match):\n",
    "#### 1. Differentiate a list of functions. \n",
    "Our package can deal with one function with multiple varaibles. In the future we plan to take a list of functions as input and output its Jacobian accordingly. Using Numpy array as the data structure to keep the Jacobian would be ideal, so we will need to change the implementation of our current jacobian method.\n",
    "\n",
    "#### 2. Higher order derivatives. \n",
    "A starting point would be allowing second order derivatives taken on our AD objects and returning the correct Jacobian matrix accordingly. Note that this cannot be achieved by simply applying `diff()` to an AD object twices, since the Jacobian matrix would be different and the datatype would be different. We would need to store the values of the second derivatives of our AD object at each elementary steps in the evaluation trace. Then we would need another function to return the second derivatives (possibly named `second_diff()`), which functions similarly to `diff()`, but returns the second derivatives of the AD object. Apart from the `jacobian()` function, we will also have a `hessian()` function which returns the second order derivatives matrix of the function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Extension and its Background\n",
    "As mentioned above, our main extension is calculating higher order derivatives using automatic differentiation. Higher order derivatives are prevalent in Numerical Analysis researches, mechanical engineering, astronomy and a number of other fields of application. Most of the times, people are interesting in using derivatives of order 1, 2 and 3, seldomly 4 and 5. Orders higher than such are rarely consider except for pure mathematical and academic purposes. More mathematical background of higher order derivatives is provided in the Background section above. \n",
    "\n",
    "Our implementation of higher order derivatives is integrated with our main object classes `VAD`. The features differ for single value input and vector input of `VAD`, so we will introduce them separately. \n",
    "\n",
    "For `VAD` and functions of `VAD` with single value input, we can calculate their derivatives up to an arbitrary order specified by the attribute `order`. These high order derivatives are stored in an attribute `higher` and can be accessed through it. Notice that due to the nature of differentiation, most derivatives will become 0 after some times of differentiations, except for the iterative functions including `sin` and `cos`. \n",
    "\n",
    "For `VAD` and functions of `VAD` with vector value input, we can calculate their derivatives up to the second order. This shall be enough for basic applications of differentiating vectors. The first and second order derivatves are stored in attributes `der` and `der2` respectively and can be accessed through them and through functions `jacobian()` and `hessian()`.\n",
    "\n",
    "In addition, we demoed a case where our extension could aid Newton's method in the file `using_VAD_for_Newtons_method.ipynb` in the `docs` directory. It is quite an integrated test scheme which utilizes a large coverage of the elementary operations that can be used in our modules. It is highly encouraged to try this test case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-ae938266-d588-44fe-8cf0-0f1e8037a12f"
   },
   "source": [
    "# Broader Impact and Inclusivity Statement\n",
    "\n",
    "### Broader Impact\n",
    "We hope our package would be applied to different fields that require doing differentiations via computer programs: physics, engineering, applied mathematics, astronomy, and even other areas that the developers of this package have never imagined. We hope this package can be used to do automatic differentiations accurately and efficiently and can inspire the development of enhanced versions of automatic differentiation packages in the future. We see a number of possibilities that this package could be enhanced and would be happy to see them completed. \n",
    "\n",
    "On the other hand, we do not hope to see that this package is used for plagiarism, cheating, or shortcut for doing differentiation. The open-source nature of this package makes it accessible to people, but also susceptible to people who plan to use it for plagiarism. Users should be aware of this nature and wisely choose their way of using this package. This package is not designed for shortcuts of doing differentiation practices. People could use it to check their answers for calculating derivatives by hand or by other algorithms, but should not use it in place of derivative calculation practices. These practices have their purposes and using this package to get the answers does not contribute to the learning process.\n",
    "\n",
    "We also see that when working on this project, we connected mathematical ideas such as Leibniz Rule and Faa di Bruno Formula to our automatic differentiation algorithms. Although this should not be the first time when people used these formulas to calculate higher-order derivatives, it was inspiring for us to do the implementation ourselves. We hope our project serves as a case where we bridge the gap between theories and applications. This experience will allow us and many students alike to keep striving for this goal and further tells that this is the best time when all kinds of knowledge come together to facilitate new discoveries.\n",
    "\n",
    "### Software Inclusivity\n",
    "The autodiffCST package and its developer welcome users who are contributors from all backgrounds and identities. We believe excellence in a collaborative project comes from trust, respect, and caring for each other, as it is evident through the process of developing this package. We tried our best to make our package as inclusive and user-friendly as possible with the willingness to reach more people that are interested in this package, by providing fitting documentation and instructions. Admittedly, this package is written in English and Python, but we welcome the contribution from people that are fluent in any language and programming languages. During the process of developing this package, pull requests are reviewed and approved by all developers. Whenever one of us feels the need to initiate a pull request, this person would communicate with other members and reach an agreement together. We would love to bring this positive communication to a future collaboration of this package and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future\n",
    "\n",
    "1. In the future, we would like our package to be able to implement higher (>2) order derivatives for vector inputs. We now can handle up to second order, but did not get to implement orders higher than that. Such orders could be useful for some applications.\n",
    "2. We would like to handle functions that contains multiple vector inputs. As of now, we can only perform operations with one vector input `VAD` object, or multiple single value input `VAD` objects. If we were to implement functions with nultiple vector inputs, the Jacobian and Hessian matrices would be much more complicated than what we have now. This could serve as a fitting extension for furture implementations.\n",
    "3. Improve computation efficiency. For instance, we used the Faa di Bruno Formula to calculate higher order derivatives, but is there a more efficient approach, in terms of both time and storage complexity? Possible candidates are using symbolic expression of the function, using Backward Mode and using other formulas. It is yet to explore which one is the most efficient option.\n",
    "4. Further applications. As mentioned above, our modules can already be used in Newton's method and fits applications in areas such as mechanical engineering and dynamic system. As of the higher order derivatve extension, it can be useful in Numerical Analysis and pedagogical purposes. Physics is another area of possible application of our package, since second order derivatives are prevalent."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "milestone1.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "7a9561a0-8feb-4a89-9b6d-4b39a18ba7a1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
